# Cue-based Acquisition of Disjunction  {#modeling}

## Introduction

In Chapter \@ref(sempragLit), I reviewed the complexities involved in interpreting a disjunction word such as *or* in English. I showed that a disjunction can be interpreted as inclusive, exclusive, and even conjunctive. In addition to these truth-conditional interpretations, a disjunction is sometimes associated with speaker ignorance/indifference as well. Given the wide range of interpretations that *or* can have, how can children learn to interpret it correctly? 
This is what the current chapter will address. In doing so, it also provides a potential solution to the puzzle of learning disjunction mentioned in the Introduction. To recap the puzzle, previous corpus research as well as the study in Chapter \@ref(corpus), have shown that the majority of *or*-examples children hear are exclusive. However, comprehension studies report that between the ages of three and five, children can interpret *or* as inclusive disjunction in declarative sentences [@crain2012emergence]. The finding of the comprehension studies and the corpus studies taken together present a learning puzzle: how can children learn to interpret *or* as inclusive if they mostly hear exclusive examples? This chapter provides a solution by developing a cue-based account for children's acquisition of connectives. More generally, the account proposed in this chapter is helpful for learning words with multiple interpretations when one interpretation dominates the learner's input. 

Learning from multiple cues is a common approach in language acquisition [see @monaghan2014multiple, for an overview]. In the domain of function word semantics, @bloom1997linguistic proposed a cue-based account for the acquisition of number words. In the next section, I briefly review their proposal and report their findings. The annotation study in Chapter \@ref(corpus) used a methodology similar to that of @bloom1997linguistic and reported several cues that may help children's acquisition of the connectives *and* and *or*. In this section, I use the data in my annotation study to present a cue-based account of connective acquisition. This account provides a straightforward solution to the learning puzzle of disjunction. I provide support for the cue-based account using three modeling experiments. The models incorporate the proposed cues to learn decision trees that predict the interpretation of a disjunction/conjunction.

## The Cue-based Account for Number Words 

Research on children's acquisition of numeral words (e.g. *one*, *two*, *three*, etc.) has suggested that children initially know that number words greater than "one" refer to precise numerosities but they do not know exactly which word refers to which number [@wynn1992children]. @bloom1997linguistic searched for linguistic cues that could help children associate numerals with quantity and numerosity. They considered two classes of cues: syntactic and semantic. Syntactic cues to word meaning were first discussed by @brown1957linguistic. He wrote: "If a part of speech has reliable semantic implications, it could call attention to the kind of attribute likely to belong to the meaning of the word ... the part of speech membership of the new word could operate as a filter selecting for attention probably relevant features of the nonlinguistic world." He tested preschoolers with nonce constructions "to sib", "a sib", and "any sib" showing that children could use the modifying function words to decide whether the nonce word *sib* should refer to an action, an object, or a substance. 

Semantic cues, on the other hand, are provided by the meaning of the known words in the sentence. Consider the sentence "there were several gloobs." The use of *gloob* in the plural noun phrase "several gloobs" makes it possible to infer that "gloob" is not an action or a spatial relationship but rather an entity that can have multiple instances. Using only the syntactic cues, there still remains a wide range of referential uncertainty since a gloob may be anything from an egg to an alien creature. Now consider the sentence "I ate several gloobs for breakfast." What a gloob may be is now restricted to edible entities, probably those that are suitable for breakfast. The meanings of the verb *eat* and the adverbial phrase "for breakfast" help further narrow down the possible meanings for *gloob*. 

It is not always easy to tell whether a cue is syntactic or semantic. Here I avoid this issue by using the term "compositional cues" to refer to both syntactic and semantic cues that aid the interpretation of an unknown word. Using the term "compositional" also brings into attention the fact that syntactic and semantic cues are interrelated and do not act in an independent or unstructured way. Consider the sentence "After eating breakfast, I saw several gloobs." Even though the words *eat* and *breakfast* are present in the sentence, they do not restrict the possible meanings for *gloob* as they did before. In other words, it is not the mere presence of these words in the sentence that act as cues but rather the way they combine with the unknown word. The phrase "compositional cues" can help us highlight such important nuances.

@bloom1997linguistic proposed that children learn number word meanings by attending to the compositional cues that accompany number words such as the words' ordering relative to other words, function words they co-occur with, and the count-mass status of the nouns they modify. They specifically discussed four cues. Two cues could help children notice that number words pattern like quantifiers. First, similar to quantifiers, number words precede adjectives and do not follow them. Second, they participate in the "... of the Xs" construction: "one of the gloobs", "some of the gloobs", "most of the gloobs", etc. The third cue is the co-occurrence of number words with count nouns. This cue can inform learners that their meaning is restricted to the quantification of individuals. Finally, unlike other adjectives, numerals cannot be modified further using an adverb such as *very* or *too* ("very big animals" vs. "\*very two animals"). According to @bloom1997linguistic, this cue can help a learner understand that number words pick an absolute property of a set rather than a continuous one.

Using the data available in the CHILDES corpora, @bloom1997linguistic investigated the presence of cues to number-word meaning in child-directed speech for three children between the ages of one and three. They found that these children and their parents only use number words with count nouns; they do not use number words with modifiers and only use them before adjectives, not after. Finally, they found that these children and their parents use only number words and quantifiers in the partitive construction and not with adjectives. The results of their corpus study show that the compositional cues they proposed for number-word acquisition are available in children's linguistic input. In the next section, I discuss some compositional cues that can help a learner limit the hypothesis space to connective meanings for coordinators such as *and*, *or*, *but*, *so*, etc.

## Cues to coordinator meanings

```{r data_prep, echo=FALSE, warning=FALSE, message=FALSE}
alex_data <- read.csv("connective_modeling/annotation_data/ProvidenceData-alex.csv", nrows=100)
lily_data <- read.csv("connective_modeling/annotation_data/ProvidenceData-lily.csv", nrows=99)
vio_data <- read.csv("connective_modeling/annotation_data/ProvidenceData-vio.csv", nrows=102)
will_data <- read.csv("connective_modeling/annotation_data/ProvidenceData-will.csv", nrows=100)
naima_data <- read.csv("connective_modeling/annotation_data/ProvidenceOR-Naima.csv", nrows=241)

naima_data <- 
  naima_data %>%
  filter(!is.na(exclusivity), !is.na(intonation))

all_data <- rbind(alex_data, lily_data, vio_data, will_data, naima_data)

all_data$intonation <- as.factor(all_data$intonation)

all_data$exclusion <- fct_recode(all_data$exclusion, "Consistent" = "ELS", "Inconsistent" = "EXC")
all_data$intonation <- fct_recode(all_data$intonation, "Flat" = "0", "Rising" = "1", "Rise-Fall" = "2")
all_data$intonation <- fct_recode(all_data$intonation, "Flat" = "0", "Rising" = "1", "Rise-Fall" = "2")
```

```{r data_wrangling, echo=FALSE, warning=FALSE, message=FALSE}
raw_data <- 
  all_data %>%
  group_by(id, exclusivity) %>%
  summarise(counts= n()) %>%
  spread(exclusivity, counts) %>%
  replace(is.na(.), 0) %>%
  mutate(total = IN + EX) %>%
  gather(EXIN, counts, EX:IN) %>%
  mutate(prop = counts / total) %>%
  group_by(EXIN) %>%
  summarize(cih = ci.high(prop),
            cil = ci.low(prop),
            prop = mean(prop))

graph_data <- 
  all_data %>%
  group_by(id, intonation, exclusion, exclusivity) %>%
  summarise(counts= n()) %>%
  spread(exclusivity, counts) %>%
  replace(is.na(.), 0) %>%
  mutate(total = EX + IN) %>%
  gather(exclusivity, counts, EX:IN) %>%
  mutate(prop = counts / total) %>%
  group_by(exclusivity, intonation, exclusion) %>%
  summarize(cih = ci.high(prop),
            cil = ci.low(prop),
            prop = mean(prop)) 

counts_table<-  
  all_data %>%
  group_by(intonation, exclusion, syn_level, exclusivity) %>%
  summarise(counts= n()) %>%
  spread(exclusivity, counts) %>%
  replace(is.na(.), 0) %>%
  mutate(total = IN + EX)

exclusivity_overall <-
  all_data %>%
  group_by(exclusivity) %>%
  summarise(counts= n()) %>%
  spread(exclusivity, counts) %>%
  replace(is.na(.), 0) %>%
  mutate(total = IN + EX) %>%
  gather(exclusivity, counts, EX:IN) %>%
  mutate(prop = counts / total)

cds_disjunction_model <- summary(glmer(exclusivity ~ intonation + exclusion + (1|id), family="binomial", data=all_data))
```

```{r importSyntaxInfo}
annotations <- read_csv("connective_learning/3_providence_annotations/providence_merged.csv")

synTable <-
annotations %>%
  group_by(syn_level) %>%
  summarize(counts=n(), total = nrow(annotations), prop = counts/total)
```

Three important compositional cues can help learners in restricting their hypotheses to coordinator meanings. First, as pointed out by @haspelmath2007, coordination has specific compositional properties. Coordinators combine two or more units of the same type and return a larger unit of the same type. The larger unit has the same semantic relation with the surrounding words as the smaller units would have had without coordination. These properties separate coordinators from other function words such as articles, quantifiers, numerals, prepositions, and auxiliaries which are not used to connect sentences or any two similar units for that matter. In fact, the special syntactic properties of coordinators have compelled syntactic theories to consider specific rules for coordination.  

The literature on syntactic bootstrapping suggests that children can use syntactic properties of the input to limit their word meaning hypotheses to the relevant domain [@brown1957linguistic; @gleitman1990structural; see @fisher2010syntactic for a review]. In the current `r nrow(annotations)` annotations of conjunction and disjunction, I found that *and* and *or* connected sentences/clauses `r round(synTable$prop[2]*100)`% of the time. This pattern is unexpected for any other class of function words and it is possible that the syntactic distribution of coordinators cue the learners to the space of sentential connective meanings. 
<!--
Second, as mentioned in Chapter \@ref(corpus) as well, the concpetual relation between the propositions connected by a connective can also cue the connective meaning. For example, the propositions "falling off a bike" and "breaking an arm" have temporal and causal relations. One often happens before the other, and one is the cause of the other. Suppose I connect these two propositions using the made-up connective *yok*: "He fell off the bike yok broke his arm." A reasonable hypothesis is that *yok* signals the temporal and/or causal connection between these two events. This is what *and* often does in English. However, if I say "He broke his arm yok he fell off the bike", it is more probably that the connective *yok* -->

Second, in the annotation study I found that *and* never occurs with inconsistent coordinands (e.g. "clean and dirty") while *or* commonly does (e.g. "clean or dirty"). The inconsistency of the coordinands can cue the learner to not consider conjunction as a meaning for the coordinator given that a conjunctive meaning would too often lead to a contradiction at the utterance level. On the other hand, choosing disjunction as the meaning avoids this problem. Third, the large scale study of Chapter \@ref(corpus) found that *or* is more likely to occur in questions than statements while *and* is more likely in statements. Since questions often contain more uncertainty while statements are more informative, it is possible that these environments bias the learner towards selecting hypotheses that match this general communicative function. Disjunction is less informative than conjunction and it is possible that the frequent appearance of *or* in questions cues learners to both its meaning as a disjunction as well as the ignorance inference commonly associated with it.

Finally, it is reasonable to assume that not all binary connective meanings shown in Figure \@ref(fig:binaryLogicalConnectivess) are as likely for mapping. For example, coordinators that communicate tautologies or contradictions seem to be not good candidates for informative communication. Similarly, if A coordinated with B simply asserts the truth of A and says nothing about B, it is unclear why it would be needed if the language already has the means of simply asserting A. It is possible that pragmatic principles already bias the hypothesis space to favor candidates that are communicatively more efficient.

```{r binaryLogicalConnectivess, fig.env="figure", fig.align="center", fig.width=5, fig.height=2, fig.cap="The truth table for the 16 binary logical connectives. The rows represent the set of situations where zero, one, or both propositions are true. The columns represent the 16 possible connectives and their truth conditions. Green cells represent true situations."}
binary_connectives<- png::readPNG("figs/binary_connective.png")
grid::grid.raster(binary_connectives)
```

Even though these findings are suggestive, they need to be backed up by further observational and experimental evidence to show that children do actually use these cues in learning connective meanings. In the next section, I turn to the more specific issue of learning the correct interpretation of *and* and *or* from the input data. As in the case of number words, previous research has provided insight into how children comprehend a disjunction and what they hear from their parents. The main question is how children learn what they comprehend from what they hear. I turn to this issue in the next section.

## Learning to interpret *and* and *or*: A cue-based account {#myaccount}

Previous comprehension studies as well as the one reported in Chapter \@ref(comprehension) have shown that children as early as age three can interpret a disjunction as inclusive. However, Morris' [-@morris2008logically] study of child-directed speech showed that exclusive interpretations are much more common than other interpretations of disjunction in children's input. In Figure \@ref(fig:interpretation), I show the results of Chapter \@ref(corpus)'s annotation study by grouping the disjunction interpretations into exclusive (EX) and inclusive (IN), i.e. non-exclusive categories. These results replicate Morris' [-@morris2008logically] finding and reinforce a puzzle raised by @crain2012emergence: How can children learn the inclusive interpretation of disjunction when the majority of the examples they hear are exclusive? To answer this question, I draw on insights from the Gricean approach to semantics and pragmatics discussed in Chapter \@ref(sempragLit). 

```{r interpretation, fig.env='figure',fig.width=1.8, fig.height=1.8, fig.align="center",fig.cap = "Proportion of exclusive and inclusive interpretations of disjunction in child-directed speech. Error bars represent bootstrapped 95\\% confidence intervals."}
ggplot(raw_data, aes(x= EXIN, y=prop, fill=EXIN)) + 
  theme_few(base_size = 10) + 
  geom_col(width=0.7) +
  geom_linerange(aes(ymax = cih, ymin = cil)) + 
  guides(fill=FALSE) +
  labs(title = "", x = "", y = "Proportion") + 
  theme(text = element_text(family = "Times"))+
  scale_colour_solarized()
```

Research in Gricean semantics and pragmatics has shown that the word *or* is not the only factor relevant to the interpretation of a disjunction. It is not only the presence of the word *or* that leads us to interpret a disjunction as inclusive, exclusive, or conjunctive, but rather the presence of *or* along with several other factors such as intonation [@pruitt2013interpretation], the meaning of the disjuncts [@geurts2006exclusive], and the conversational principles governing communication [@grice1989studies]. The interpretation and acquisition of the word *or* cannot, therefore, be separated from all the factors that accompany it and shape its final interpretation. 

In the literature on word learning and semantic acquisition, form-meaning mapping is often construed as mapping an isolated form such as *gavagai* to an isolated concept such as "rabbit". While this approach may be feasible for content words, it will not work for function words such as *or*. First, the word *or* cannot be mapped in isolation from its formal context. As @pruitt2013interpretation showed, the intonation that accompanies a disjunction affects its interpretation. Therefore, a learner needs to pay attention to the word *or* as well as the intonation contour that accompanies it. Second, the word *or* cannot be mapped to its meaning isolated from the semantics of the disjuncts that accompany it. As @geurts2006exclusive argued, the exclusive interpretation is often enforced simply because the options are incompatible. For example, "to be or not to be" is exclusive simply because one cannot both be and not be. In addition, conversational factors play an important role in the interpretation of *or* as @grice1989studies argued. In sum, the interpretation and acquisition of function words such as *or* require the learner to consider the linguistic and nonlinguistic context of the word and map the meanings accordingly.

Previous accounts have adopted a model in which a function word such as *or* is mapped directly to its most likely interpretation: 

*or* $\rightarrow \oplus$

This model is often used in cross-situational accounts of content words. Here I argue that the direct mapping of *or* to its interpretation without consideration of its linguistic context is the primary cause of the learning puzzle for *or*. Instead, I propose that the word *or* is mapped to an interpretation in a context-dependent manner, along with the interpretive cues that accompany it such as intonation and disjunct semantics: 

[connective: *or*, Intonation: rise-fall, Disjuncts: inconsistent] $\rightarrow \oplus$

[connective: *or*, Intonation: rising, Disjuncts: consistent] $\rightarrow \lor$

Figure \@ref(fig:interpretationByIntonationAndConsistency) shows that the rate of exclusive interpretations change systematically when the data are broken down by intonation and consistency. Given a rise-fall intonation contour, a disjunction is almost always interpreted as exclusive. Similarly, if the propositions are inconsistent, the disjunction is most likely interpreted as exclusive. When either of these two features are absent, a disjunction is more likely to receive an inclusive interpretation.

```{r interpretationByIntonationAndConsistency, fig.env='figure', fig.width=3.5, fig.height=2.5, fig.align="center", fig.cap = "Exclusive and inclusive interpretations broken down by intonation (flat, rise, rise-fall) and consistency. Error bars represent bootstrapped 95\\% confidence intervals."}
ggplot(graph_data, aes(x= exclusivity, y=prop, fill=exclusivity)) + 
  geom_col(width=0.7) +
  geom_linerange(aes(ymax = cih, ymin = cil)) +
  guides(fill=FALSE) +
  theme_few(base_size = 10) + 
  labs(title = "", x = "", y = "Proportion") + 
  scale_colour_solarized() + facet_grid(exclusion~intonation) + theme(text = element_text(family = "Times"))
```

In this account, it is not a single word that gets mapped to an interpretation but rather a cluster of features. This method has two advantages. First, it deals with the context dependency of disjunction interpretation. The learner knows that *or* with some intonation has to be interpreted differently from one with another. Second, it allows the learner to pull apart the contribution of *or* from the interpretive cues that often accompany it. In fact, analysis of all mapping clusters in which *or* participates and generalization over them can help the learner extract the semantics of *or* the way it is intended by Gricean accounts of semantics/pragmatics. For those skeptical of such an underlying semantics for *or*, there is no need for further analysis of the mapping clusters. The meaning of *or* as a single lexical item is distributed among the many mappings in which it participates. In the next section, I implement this idea using decision tree learning.

## Modeling Using Decision Tree Learning {#DecisionTrees}

A decision tree is a classification model structured as a hierarchical tree with nodes, branches, and leaves [@breiman2017classification]. The tree starts with an initial node, called the root, and branches into more nodes until it reaches the leaves. Each node represents the test on a feature, each branch represents an outcome of the test, and each leaf represents a classification label. Using a decision tree, observations can be classified or labeled based on a set of features. For example, we can make a decision tree to predict whether a food item is a fruit or not based on its color (green or not) and shape (round or not). An example decision tree is the following: at the root, the model can ask whether the item is green or not. If yes, the model creates a leaf and labels the item as "not fruit". If not, the model creates another node and asks if the item is round. If yes, the item is classified as a "fruit" and if not it is classified as "not fruit". 

Decision trees have several advantages for modeling cue-based accounts of semantic acquisition. First, decision trees use a set of features to predict the classification of observations. This is analogous to using cues to predict the correct interpretation of a word or an utterance. Second, unlike many other machine learning techniques, decision trees result in models that are interpretable. Third, the order of decisions or features used for classification is determined based on information gain. Features that appear higher (earlier) in the tree are more informative and helpful for classification. Therefore, decision trees can help us understand which cues are probably more helpful for the acquisition and interpretation of a word.

Decision tree learning is the construction of a decision tree from labeled training data. This section applies decision tree learning to the annotated data of Chapter \@ref(corpus) by constructing random forests [@ho1995random; @breiman2001random]. In random classification forests multiple decision trees are constructed on subsets of the data and the decisions are made by taking the majority vote. Next section discusses the methods used in constructing the random forests for interpreting connectives *or*/*and*.

### Methods

The random forest models were constructed using python's Sci-kit Learn package [@pedregosa2011scikit]. The annotated data had a feature array and a connective interpretation label for each connective use. Connective interpretations included exclusive (XOR), inclusive (IOR), conjunctive (AND), negative inclusive (NOR), and NPQ which states that only the second proposition is true. The features or cues used included all other annotation categories: intonation, consistency, syntactic level, utterance type, and communicative function. All models were trained with stratified 10-Fold cross-validation to reduce overfitting. Stratified cross-validation maintains the distribution of the initial data in the random sampling to build cross validated models. Maintaining the data distribution ensures a more realistic learning environment for the forests. First a grid search was run on the hyperparamter space to establish the number of trees in each forest and the maximum tree depth allowable. The default number of trees for the forests was set to 20, with a max depth of eight and a minimum impurity decrease (i.e. gini decrease) of 0. Decision trees were fit with high and low minimum gini decrease values. High minimum gini decrease results in a tree that does not use any features for branching. Such a tree represents the baseline or traditional approach to mapping that directly maps a word to its most likely interpretation. Low minimum gini decrease allows for a less conservative tree that uses multiple cues/features to predict the interpretation of a disjunction. Such a tree represents the cue-based context-sensitive account of word learning discussed in the previous section. 

### Results

I first present the results of the random forests in the binary classification task. The models were trained to classify exclusive and inclusive interpretations of disjunction. Figure \@ref(fig:binaryBaseline) shows the best performing decision tree with high minimum gini decrease. As expected, a learner that does not use any cues would interpret *or* as exclusive all the time. This is the baseline model. Figure \@ref(fig:binaryCueBased) shows the best performing decision tree with low minimum gini decrease. The tree has learned to use intonation and consistency to classify disjunctions as exclusive or inclusive. As expected, if the intonation is rise-fall or the disjuncts are inconsistent, the interpretation is exclusive. Otherwise, the disjunction is classified as inclusive. 

```{r binaryBaseline, fig.asp=0.4, fig.cap="Baseline tree grown with minimum impurity decrease of 0.2. The tree always classifies examples of disjunction as exclusive."}
binaryBaseline <- readJPEG("connective_modeling/exin_baselineTree.jpg")
grid::grid.raster(binaryBaseline)
```

```{r binaryCueBased, fig.cap="Cue-based tree grown with minimum impurity decrease of 0.01. The tree classifies examples of disjunction with rise-fall intonation as exclusive (intonation > 1.5). If the intonation is not rise-fall but the disjuncts are inconsistent (consistency < 0.5), then the disjunction is still classified as exclusive. However, if neither of these two hold, the disjunction is classified as inclusive."}
binaryCueBased <- readJPEG("connective_modeling/exin_cueBasedTree.jpg")
grid::grid.raster(binaryCueBased)
```

Figure \@ref(fig:XorBinary) shows the average F1 scores of the baseline and cue-based models in classifying exclusive examples. The models perform relatively well and similar to each other, but the cue-based model performs slightly better. The real difference between the baseline model and the cue-based model is in their performance on inclusive examples. Figure \@ref(fig:IorBinary) shows the F1 score of the forests as a function of the training size in classifying inclusive examples. As expected, the baseline model performs very poorly while the cue-based model does a relatively good job and improves with more examples.

```{r XorBinary, fig.cap="The average F1 score for class XOR (exclusive) as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
XorBinary <- readPNG("connective_modeling/ex-exin.png")
grid::grid.raster(XorBinary)
```

```{r IorBinary, fig.cap="The average F1 score for class IOR (inclusive) as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
IorBinary <- readPNG("connective_modeling/in-exin.png")
grid::grid.raster(IorBinary)
```

Next, I use decision tree learning in a ternary classification task. The model uses features to interpret a coordination with *and* and *or* as inclusive (IOR), exclusive (XOR), or conjunctive (AND). Figure \@ref(fig:ternaryBaseline) shows the baseline decision tree with high minimum gini decrease, which only uses the presence of the words *or*/*and* to interpret conjunction and disjunction. As expected, the tree interprets a coordination with *and* as a conjunction and one with *or* as exclusive disjunction. Figure \@ref(fig:ternaryCueBased) shows the cue-based decision tree with low minimum gini decrease. In addition to the presence of *and* and *or*, the tree uses intonation, consistency, communicative function, and utterance type to distinguish exclusive, inclusive, and conjunctive uses of disjunction. In short, a disjunction that is rise-fall, inconsistent, or has a conditional communicative function is classified as exclusive. Otherwise the disjunction is classified as inclusive. The tree also finds conjunctive interpretations of disjunction more likely in declarative sentences than interrogatives.

```{r ternaryBaseline, fig.asp=0.6, fig.cap="The baseline tree grown on conjunctions and disjunctions with minimum impurity decrease of 0.2. The tree uses the words \\textit{and/or} and classifies them as conjunction and exclusive disjunction respectively."}
ternaryBaseline <- readPNG("connective_modeling/intermediate_baselineTree.png")
grid::grid.raster(ternaryBaseline)
```

```{r ternaryCueBased, fig.asp=2.5, fig.cap="The cue-based tree grown on conjunctions and disjunctions with minimum impurity decrease of 0.01. After using the words \\textit{and/or}, the tree uses intonation, consistency, and the conditional communicative function to classify a large number of exclusive cases. Then it uses utterance type (interrogative) to label inclusive cases."}
ternaryCueBased <- readPNG("connective_modeling/intermediate_cueBasedTree.png")
grid::grid.raster(ternaryCueBased)
```

Figure \@ref(fig:ANDintermediate) shows the average F1 score of the conjunctive interpretations (AND) for the baseline and the cue-based models. Since the vast majority of the conjunctive interpretations are predicted by the presence of the word *and*, the baseline and cue-based models show similar performances. Setting aside conjunction examples, Figure \@ref(fig:ANDintermediateDis) shows the average F1 score of the AND interpretation of disjunction only. Here we see that the cue-based model performs better than the default model in guessing conjunctive interpretations of disjunction. The informal analysis of the trees suggest that the model does this by using the "speech act" cue. Figure \@ref(fig:XORintermediate) shows the average F1-score of the exclusive interpretations (XOR) for the baseline and the cue-based models. The cue-based model does slightly better than the baseline model. As before, the most important improvement comes in identifying inclusive examples. Figure \@ref(fig:IORintermediate) shows the average F1-score of the inclusive interpretations (IOR) for both baseline and cue-based models. The baseline model performs very poorly while the cue-based model is capable of classifying inclusive examples as well.

```{r ANDintermediate, fig.cap="The average F1 score for class AND as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
ANDintermediate <- readPNG("connective_modeling/and-intermediate.png")
grid::grid.raster(ANDintermediate)
```

```{r ANDintermediateDis, fig.cap="The average F1 score for class AND of disjunction examles as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
ANDintermediateDis <- readPNG("connective_modeling/and-intermediate-disjunction.png")
grid::grid.raster(ANDintermediateDis)
```

```{r XORintermediate, fig.cap="The average F1 score for class XOR as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
XORintermediate <- readPNG("connective_modeling/xor-intermediate.png")
grid::grid.raster(XORintermediate)
```

```{r IORintermediate, fig.cap="The average F1 score for class IOR as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
IORintermediate <- readPNG("connective_modeling/ior-intermediate.png")
grid::grid.raster(IORintermediate)
```

Finally, I look at decision trees trained on the annotation data to predict all the interpretation classes for disjunction: AND, XOR, IOR, NOR, and NPQ. Figure \@ref(fig:wholeBaseline) shows the baseline model that only uses the words *and* and *or* to classify. As expected, *and* receives a conjunctive interpretation (AND) and *or* receives an exclusive interpretation (XOR). Figure \@ref(fig:wholeCueBased) shows the best example tree of the cue-based model. The leaves of the tree show that it recognizes exclusive, inclusive, conjunctive, and even negative inclusive (NOR) interpretations of disjunction. How does the tree achieve that? Like the baseline model, the tree first asks about the connective used: *and* vs. *or*. Then like the previous models, it asks about intonation and consistency. If the intonation is rise-fall, or the disjuncts are inconsistent, the interpretation is exclusive. Then it asks whether the sentence is an interrogative or a declarative. If interrogative, it guesses an inclusive interpretation. This basically covers questions with a rising intonation. Then the tree picks declarative examples that have conditional speech act (e.g. "give me the toy or you're grounded") and labels them as exclusive. Finally, if negation is present in the sentence, the tree labels the disjunction as NOR. 

```{r wholeBaseline, fig.cap="The baseline tree grown on conjunctions and disjunctions with minimum impurity decrease of 0.2. The tree uses the words \\textit{and/or} and classifies them as conjunction and exclusive disjunction."}
wholeBaseline <- readPNG("connective_modeling/whole_baselineTree.png")
grid::grid.raster(wholeBaseline)
```

```{r wholeCueBased, fig.asp=4, fig.cap="The cue-based tree grown on conjunctions and disjunctions with minimum impurity decrease of 0.01. After using the words \\textit{and/or}, the tree uses intonation and consistency to classify a large number of exclusive cases. Then it uses utterance type (interrogative) to label many inclusive cases, as well as the communicative function (conditional) to catch more exclusive examples. Finally, it asks whether the sentence has negation or not. If so, it classifies the negative inlusive examples as NOR."}
wholeCueBased <- readPNG("connective_modeling/whole_cueBasedTree.png")
grid::grid.raster(wholeCueBased)
```

Figures \@ref(fig:ANDWhole), \@ref(fig:XORWhole), and \@ref(fig:IORWhole) show the average F1-scores for the conjunctive (AND), exclusive (XOR), and inclusive (IOR) interpretations as a function of training size. The results are similar to what I reported before with the ternary classification. While the cue-based model generally performs better than the baseline model, it shows substantial improvement in classifying inclusive cases. 

```{r ANDWhole, fig.cap="The average F1 score for class AND as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
AndWhole <- readPNG("connective_modeling/and-whole.png")
grid::grid.raster(AndWhole)
```

```{r XORWhole, fig.cap="The average F1 score for class XOR as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
XorWhole <- readPNG("connective_modeling/xor-whole.png")
grid::grid.raster(XorWhole)
```

```{r IORWhole, fig.cap="The average F1 score for class IOR as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
IorWhole <- readPNG("connective_modeling/ior-whole.png")
grid::grid.raster(IorWhole)
```

Figure \@ref(fig:NORWhole) shows the average F1-score for the negative inclusive interpretation as a function of training size. Compared to the baseline model, the cue-based model shows a substantially better performance in classifying negative sentences. The success of the model in classifying negative inclusive examples (NOR) suggests that the cue-based model offers a promising approach for capturing the scope relation of operators such as negation and disjunction. Here, the model learns that when negation and disjunction are present, the sentence receives a negative inclusive (NOR) interpretation. In other words, the model has learned the narrow-scope interpretation of negation and disjunction from the input data. In a language where negation and disjunction receive an XOR interpretation (not A or not B), the cue-based model can learn the wide-scope interpretation of disjunction. 

```{r NORWhole, fig.cap="The average F1 score for class NOR as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
NorWhole <- readPNG("connective_modeling/nor-whole.png")
grid::grid.raster(NorWhole)
```

Finally, Figure \@ref(fig:NPQWhole) shows the average F1 score for the class NPQ. This interpretation suggested that the first disjunct is false but the second true. It was seen in examples of repair most often and the most likely cue to it was also the communicative function or speech act of repair. The results show that even though there were improvements in the cue-based model, they were not stable as shown by the large confidence intervals. It is possible that with larger training samples, the cue-based model can reliably classify the NPQ interpretations as well.

```{r NPQWhole, fig.cap="The average F1 score for class NPQ as a function of the number of training examples in the baseline and cue-based models. The colored shades show the 95% confidence intervals."}
NpqWhole <- readPNG("connective_modeling/npq-whole.png")
grid::grid.raster(NpqWhole)
```

## Discussion

In this chapter, I discussed two accounts for the acquisition of function words. The first account was a baseline (context-independent) account that is used in vanilla cross-situational word learning: words are isolated and directly mapped to their most frequent meanings. The second account is what I called the cue-based context-dependent mapping in which words are mapped to meanings conditional on a set of present cues in the context. I argued that the puzzle of learning disjunction arises because in the baseline account, forms are mapped directly to meanings without considering the context of use. Under this account, the input statistics supports an exclusive interpretation for *or*. However, comprehension studies show that children can interpret *or* as inclusive. I showed that the cue-based account resolves this problem by allowing *or* to be mapped to its interpretation according to the set of contextual cues that disambiguate it. The results of computational experiments with decision tree learning on data from child-directed speech suggested that such an approach can successfully learn to classify a disjunction is inclusive or exclusive. More broadly, cue-based context-dependent mapping is useful for the acquisition of ambiguous words and interpretations that are consistent but relatively infrequent in child-directed speech. 